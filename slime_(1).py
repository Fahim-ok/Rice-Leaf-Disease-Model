# -*- coding: utf-8 -*-
"""slime (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jpxtLz1HFqHzsNrmrPT3RqAngEmVxBSv
"""

# # This Python 3 environment comes with many helpful analytics libraries installed
# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# # For example, here's several helpful packages to load

# import numpy as np # linear algebra
# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# # Input data files are available in the read-only "../input/" directory
# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

# import os
# for dirname, _, filenames in os.walk('/kaggle/input'):
#     for filename in filenames:
#         print(os.path.join(dirname, filename))

# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt

# Path to the folder containing class subfolders
folder_path = "/kaggle/input/bd-rice-leaf-dataset/Field Background"

# Initialize an empty dictionary to store class mean images
class_mean_images = {}

# Define the common size to resize the images
common_size = (256, 256)

# Iterate over each subfolder (class folder)
for class_folder in os.listdir(folder_path):
    class_path = os.path.join(folder_path, class_folder)
    class_images = []

    # Iterate over each image in the class
    for image_name in os.listdir(class_path):
        image_path = os.path.join(class_path, image_name)
        # Load and resize the image
        image = cv2.imread(image_path)
        resized_image = cv2.resize(image, common_size)
        class_images.append(resized_image)

    # Compute the mean image for the class
    class_mean_image = np.mean(class_images, axis=0)
    # Convert to uint8 to display the image correctly
    class_mean_image = class_mean_image.astype(np.uint8)

    # Store the mean image in the dictionary
    class_mean_images[class_folder] = class_mean_image

# Visualize sample mean images
plt.figure(figsize=(15, 10))
for i, (class_name, mean_image) in enumerate(class_mean_images.items(), start=1):
    plt.subplot(5, 8, i)
    plt.imshow(cv2.cvtColor(mean_image, cv2.COLOR_BGR2RGB))
    plt.title(class_name)
    plt.axis('off')

plt.tight_layout()
plt.show()

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt

# Path to the folder containing class subfolders
folder_path = "/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/malignant/malignant (1).png"

# Initialize an empty dictionary to store class mean images
class_mean_images = {}

# Define the common size to resize the images
common_size = (256, 256)

# Iterate over each subfolder (class folder)
for class_folder in os.listdir(folder_path):
    class_path = os.path.join(folder_path, class_folder)
    class_images = []

    # Iterate over each image in the class
    for image_name in os.listdir(class_path):
        image_path = os.path.join(class_path, image_name)
        # Load and resize the image
        image = cv2.imread(image_path)
        resized_image = cv2.resize(image, common_size)
        class_images.append(resized_image)

    # Compute the mean image for the class
    class_mean_image = np.mean(class_images, axis=0)
    # Convert to uint8 to display the image correctly
    class_mean_image = class_mean_image.astype(np.uint8)

    # Store the mean image in the dictionary
    class_mean_images[class_folder] = class_mean_image

# Visualize sample mean images
plt.figure(figsize=(15, 10))
for i, (class_name, mean_image) in enumerate(class_mean_images.items(), start=1):
    plt.subplot(5, 8, i)
    plt.imshow(cv2.cvtColor(mean_image, cv2.COLOR_BGR2RGB))
    plt.title(class_name)
    plt.axis('off')

plt.tight_layout()
plt.show()



import numpy as np
import cv2
import os

def calc_avg_mean_std(img_names, img_root, size):
    mean_sum = np.array([0., 0., 0.])
    std_sum = np.array([0., 0., 0.])
    n_images = len(img_names)
    for img_name in img_names:
        img = cv2.imread(img_root + img_name)
        img = cv2.resize(img, size)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        mean, std = cv2.meanStdDev(img)
        mean_sum += np.squeeze(mean)
        std_sum += np.squeeze(std)
    return (mean_sum / n_images, std_sum / n_images)

import os
import cv2
import numpy as np

def calc_avg_mean_std(img_names, img_root, size):
    mean_values = []
    std_values = []
    for img_name in img_names:
        img_path = os.path.join(img_root, img_name)
        # Check if the file exists
        if not os.path.exists(img_path):
            print(f"File {img_path} not found.")
            continue
        img = cv2.imread(img_path)
        # Check if the image is loaded successfully
        if img is None:
            print(f"Error loading image: {img_path}")
            continue
        # Check if the image is empty
        if img.size == 0:
            print(f"Empty image: {img_path}")
            continue
        img = cv2.resize(img, size)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        mean, std = cv2.meanStdDev(img)
        mean_values.append(mean)
        std_values.append(std)
    mean_values = np.asarray(mean_values)
    std_values = np.asarray(std_values)
    avg_mean = np.mean(mean_values, axis=0)
    avg_std = np.mean(std_values, axis=0)
    return avg_mean, avg_std

train_img_root = '/kaggle/input/bd-rice-leaf-dataset/Field Background'
train_img_names = os.listdir(train_img_root)
train_mean, train_std = calc_avg_mean_std(train_img_names, train_img_root, (512, 512))
print("Mean:", train_mean)
print("Std Deviation:", train_std)

import os
import numpy as np
from PIL import Image

# Directory containing the images
dataset_dir = "/kaggle/input/bd-rice-leaf-dataset/Field Background/Leaf Scaled"

# Initialize lists to store image arrays
image_arrays = []

# Define target image dimensions
target_width = 256
target_height = 256

# Load images, resize, and convert them to arrays
for filename in os.listdir(dataset_dir):
    if filename.endswith(".jpg"):
        image_path = os.path.join(dataset_dir, filename)
        image = Image.open(image_path)
        # Resize image to target dimensions
        resized_image = image.resize((target_width, target_height))
        # Convert image to grayscale and then to numpy array
        image_array = np.array(resized_image.convert("L"))
        image_arrays.append(image_array)
        print(f"Loaded and resized image: {filename}")

if not image_arrays:
    print("No images loaded. Please check the directory path.")
else:
    # Convert list of arrays to a single numpy array
    image_stack = np.stack(image_arrays)

    # Compute mean and standard deviation across all images
    mean_image = np.mean(image_stack, axis=0)
    std_dev_image = np.std(image_stack, axis=0)

    # Print mean and standard deviation
    print("Mean:")
    print(mean_image)
    print("\nStandard Deviation:")
    print(std_dev_image)

import os
import numpy as np
from PIL import Image

# Directory containing the images
dataset_dir = "/kaggle/input/bd-rice-leaf-dataset/Field Background"

# Initialize lists to store image arrays
all_class_arrays = {}

# Define target image dimensions
target_width = 256
target_height = 256

# Iterate over each class directory
for class_folder in os.listdir(dataset_dir):
    class_dir = os.path.join(dataset_dir, class_folder)
    if os.path.isdir(class_dir):
        # Initialize list to store image arrays for this class
        image_arrays = []
        # Load images, resize, and convert them to arrays
        for filename in os.listdir(class_dir):
            if filename.endswith(".jpg"):
                image_path = os.path.join(class_dir, filename)
                image = Image.open(image_path)
                # Resize image to target dimensions
                resized_image = image.resize((target_width, target_height))
                # Convert image to grayscale and then to numpy array
                image_array = np.array(resized_image.convert("L"))
                image_arrays.append(image_array)
                print(f"Loaded and resized image: {filename} from class: {class_folder}")
        if image_arrays:
            # Convert list of arrays to a single numpy array
            class_stack = np.stack(image_arrays)
            # Compute mean and standard deviation across all images in this class
            mean_image = np.mean(class_stack, axis=0)
            std_dev_image = np.std(class_stack, axis=0)
            # Store mean and standard deviation in dictionary
            all_class_arrays[class_folder] = {"Mean": mean_image, "Std Dev": std_dev_image}

# Print mean and standard deviation for each class
for class_folder, stats in all_class_arrays.items():
    print(f"\nClass: {class_folder}")
    print("Mean:")
    print(stats["Mean"])
    print("\nStandard Deviation:")
    print(stats["Std Dev"])







train_mean / 255., train_std / 255.





!pip install -q efficientnet
!pip install keras_cv_attention_models
!pip install -q efficientnet >> /dev/null

import math, re, os
import tensorflow as tf, tensorflow.keras.backend as K
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from kaggle_datasets import KaggleDatasets
import efficientnet.tfkeras as efn
import cv2
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from matplotlib import pyplot as plt
from sklearn import metrics
from skimage.segmentation import mark_boundaries
from sklearn.metrics import confusion_matrix
import time


import numpy as np
import matplotlib.pyplot as plt
import os
import cv2
from tensorflow import keras
import seaborn as sns
import tensorflow as tf
# import tensorflow_addons as tfa
from tensorflow.keras.layers import Input, Dense, Conv2D, Activation, MaxPool2D, GlobalAveragePooling2D
from tensorflow.keras.layers import BatchNormalization, Flatten, Reshape, Conv2DTranspose, LeakyReLU
from tensorflow.keras.models import Model
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam, RMSprop
from sklearn.utils import compute_class_weight
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# AUTO = tf.data.experimental.AUTOTUNE
# GCS_PATH = KaggleDatasets().get_gcs_path('test-lime-dataset')
# IMAGE_SIZE = [256, 256]
# BATCH_SIZE = 64
# VALIDATION_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/train/.tfrec')
# DEFAULT_NUM_SAMPLES = 100
# print(VALIDATION_FILENAMES)

import tensorflow as tf
from kaggle_datasets import KaggleDatasets

AUTO = tf.data.experimental.AUTOTUNE
GCS_PATH = KaggleDatasets().get_gcs_path('wheat-leaf-rust-prepo')
IMAGE_SIZE = [112, 112]
BATCH_SIZE = 64
DEFAULT_NUM_SAMPLES = 100

# Change this line to search for image files in subdirectories
VALIDATION_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test/*/.jpg')  # Adjust '*.jpg' if your images are in a different format

#print(VALIDATION_FILENAMES)

# EFNS = [efn.EfficientNetB0, efn.EfficientNetB1, efn.EfficientNetB2, efn.EfficientNetB3,
#         efn.EfficientNetB4, efn.EfficientNetB5, efn.EfficientNetB6]
# import efficientnet.tfkeras as efn

# def build_model(dim=128, ef=0):
#     inp = tf.keras.layers.Input(shape=(dim,dim,3))
#     base = EFNS[ef](input_shape=(dim,dim,3),weights='imagenet',include_top=False)
#     x = base(inp)
#     x = tf.keras.layers.GlobalAveragePooling2D()(x)
#     x = tf.keras.layers.Dense(1,activation='sigmoid')(x)
#     model = tf.keras.Model(inputs=inp,outputs=x)
#     opt = tf.keras.optimizers.Adam(learning_rate=0.001)
#     loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05)
#     model.compile(optimizer=opt,loss=loss,metrics=['AUC'])
#     return model

# model = build_model(dim=IMAGE_SIZE[0],ef=6)


from keras_cv_attention_models import beit

# Load CoatNet and Beit models
mm2 = beit.BeitBasePatch16(input_shape=(112, 112, 3))

# Freeze fewer layers in the Beit model
num_layers_to_freeze = len(mm2.layers) * 6 // 7  # Freeze half of the layers
for layer in mm2.layers[:num_layers_to_freeze]:
    layer.trainable = False

# Create custom models
mm2_last_layer = mm2.get_layer('out_ln').output
mm2_custom = Model(mm2.input, mm2_last_layer)

# Ensemble Model
inputs = Input(shape=(112, 112, 3))
output2 = mm2_custom(inputs)
# outputs = Average()([output2])

avg_ensemble_model = Model(inputs=inputs, outputs=output2)

# Add final classification layer
num_classes = 5
output_layer = Dense(num_classes, activation='softmax', name='output_1')(avg_ensemble_model.output)
model = Model(inputs=avg_ensemble_model.input, outputs=output_layer)

# Model Summary
model.summary()


model.load_weights('/kaggle/input/biet-model-plantdiease/Best_DenseNet201rice.h5')

# List all layers in the model
for i, layer in enumerate(model.layers):
    print(i, layer.name, layer.output_shape)

# For a transformer model like BEiT, you might not find a traditional conv layer,
# but look for the last attention or Transformer block layer before the global pooling operation.

!pip install -q git+https://github.com/marcotcr/lime.git
from lime import lime_image

from sklearn.model_selection import KFold

def decode_image(image):
    image = tf.image.decode_jpeg(image, channels=3)
    image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0,1]
    image = tf.image.resize(image, [112, 112])  # Resize to model's input size
    return image


def read_jpeg_file(filename, label):
    image = tf.io.read_file(filename)
    image = decode_image(image)
    return image, label

def load_dataset(filenames, labeled=True, ordered=False):
    labels = [1] * len(filenames)  # Dummy labels, adjust as needed
    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))
    if not ordered:
        dataset = dataset.shuffle(buffer_size=1024)
    dataset = dataset.map(read_jpeg_file, num_parallel_calls=AUTO)
    return dataset

def get_dataset(files, augment=False, shuffle=False, repeat=False,
                labeled=True, return_image_names=False, batch_size=16, dim=256):
    dataset = load_dataset(files, labeled)

    if repeat:
        dataset = dataset.repeat()

    if shuffle:
        dataset = dataset.shuffle(1024 * 8)

#     if augment:
        # Add your augmentation logic here (if needed)

    if return_image_names:
        dataset = dataset.map(lambda image, label: (image, files, label),
                              num_parallel_calls=AUTO)
    else:
        # Return only images for prediction
        dataset = dataset.map(lambda image, label: image,
                              num_parallel_calls=AUTO)

    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(AUTO)

    return dataset


# Example usage of get_dataset
SEED = 42
FOLDS = 5
REPLICAS = 1
TTA = 11

GCS_PATH='/kaggle/input/chest-xray-pneumonia/chest_xray'
skf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)
for fold, (idxT, idxV) in enumerate(skf.split(np.arange(15))):
    if fold == 0:
        file_patterns = ['/kaggle/input/chest-xray-pneumonia/chest_xray/train/PNEUMONIA/person1000_virus_1681.jpeg' for x in idxV]
        files_valid = []
        for pattern in file_patterns:
            files_valid.extend(tf.io.gfile.glob(pattern))

# Function to count the number of images
def count_images(filenames):
    return len(filenames)


NUM_VALIDATION_IMAGES = count_images(files_valid)
print('Dataset: {} validation images'.format(NUM_VALIDATION_IMAGES))

# %%time
# N_SAMPLE = 2000
# ds = get_dataset(files_valid, batch_size=BATCH_SIZE)
# predictions = model.predict(next(iter(ds)))

# ds = get_dataset().unbatch().batch(N_SAMPLE).map(lambda image,image_name, target: image_name)
# image_names = next(iter(ds)).numpy().astype('U')
# ds = get_dataset().unbatch().batch(N_SAMPLE).map(lambda image,image_name, target: target)
# targets = next(iter(ds)).numpy().astype('int32')

IMAGE_SIZE = [112, 112]
BATCH_SIZE = 64
DEFAULT_NUM_SAMPLES = 100


# For predictions
N_SAMPLE = 200
ds_for_prediction = get_dataset(files_valid, labeled=False, batch_size=BATCH_SIZE)
predictions = model.predict(next(iter(ds_for_prediction)))

# For extracting image names and targets
ds_with_names = get_dataset(files_valid, return_image_names=True, batch_size=N_SAMPLE)
images, image_names, targets = next(iter(ds_with_names))  # Corrected unpacking
image_names = image_names.numpy().astype('U')
targets = targets.numpy().astype('int32')

print(predictions)



# def segment_fn(image):
#     return slic(image, n_segments=50, compactness=10, sigma=1)

# DIM = IMAGE_SIZE[0]
# circle = 1-cv2.circle((np.ones([DIM, DIM, 3])).astype(np.uint8),(DIM//2, DIM//2),np.random.randint(DIM//2 - 3, DIM//2 + 5),
#                     (0, 0, 0),-1)
# def get_explanations(image_names, num_samples=  DEFAULT_NUM_SAMPLES, random_state = 0, progress_bar = False):

#     n_img = len(image_names)
#     id_img = 1

#     for image_name in image_names:
#         explainer = lime_image.LimeImageExplainer(random_state = random_state)

#         img = cv2.imread('/kaggle/input/melanoma-merged-external-data-512x512-jpeg/512x512-dataset-melanoma/512x512-dataset-melanoma/'+image_name+'.jpg', cv2.IMREAD_UNCHANGED)
#         width = IMAGE_SIZE[0]
#         height = IMAGE_SIZE[1]
#         dim = (width, height)
#         img = cv2.resize(img, dim)
#         img = img/255.0

#         if df[df['image_name'].isin([image_name])].shape[0]==0:# if case wasnt in validation data
#             prob = np.concatenate(model.predict(tf.expand_dims(img,axis = 0)))
#             prd = np.round(prob)
#             df2  = pd.read_csv("/kaggle/input/melanoma-merged-external-data-512x512-jpeg/marking.csv")
#             target =df2[df2['image_id'].isin([image_name])][['target']].values[0][0]
#         else:
#             prd = df[df['image_name'].isin([image_name])][['pred']].values[0][0]
#             target = df[df['image_name'].isin([image_name])][['target']].values[0][0]

#         plot_num_cols = 4
#         sp = plt.subplot(n_img, plot_num_cols,id_img)
#         id_img+=1
#         title = 'original (target = '+str(target)+')'
#         sp.set_title(title)
#         sp.set_ylabel(image_name+'.jpg')
#         sp.imshow(img)

#         #sp = plt.subplot(n_img, plot_num_cols,id_img)
#         #id_img+=1
#         #title = 'transformed (microscope)'
#         #img*=circle
#         #sp.set_title(title)
#         #sp.imshow(img)

#         sp = plt.subplot(n_img, plot_num_cols,id_img)
#         id_img+=1
#         title = 'segmentation (prediction = '+str(prd)+')'

#         sp.set_title(title)
#         sp.imshow(mark_boundaries(img, segment_fn(img)))

#         explanation = explainer.explain_instance(img, model.predict, num_samples=num_samples, segmentation_fn = segment_fn,progress_bar=progress_bar)
#         temp, mask = explanation.get_image_and_mask(0, positive_only=False,  num_features=5, hide_rest=False, min_weight=0.0)
#         sp = plt.subplot(n_img, plot_num_cols,id_img)
#         sp.set_title('positive and negative regions')
#         id_img+=1
#         sp.imshow(mark_boundaries(temp, mask))

#         temp, mask = explanation.get_image_and_mask(0, positive_only=True if round(prd) == 1 else False, negative_only = True if round(prd) == 0 else False,  num_features=1, hide_rest=False, min_weight=0.0)
#         sp = plt.subplot(n_img, plot_num_cols,id_img)
#         sp.set_title('top '+ ('positive' if round(prd) == 1 else 'negative') + ' region')
#         id_img+=1
#         sp.imshow(mark_boundaries(temp, mask))

# img_list = ['ISIC_2637011']
# plt.rcParams['figure.figsize'] = [18, 5*len(img_list)]
# get_explanations(img_list, num_samples=  DEFAULT_NUM_SAMPLES, random_state = 0, progress_bar = True)

import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import cv2
from skimage.segmentation import slic
from skimage.segmentation import mark_boundaries

def segment_fn(image):
    return slic(image, n_segments=50, compactness=10, sigma=1)

def get_explanations(image_names, num_samples=DEFAULT_NUM_SAMPLES, random_state=0):
    n_img = len(image_names)
    id_img = 1

    for image_name in image_names:
        explainer = lime_image.LimeImageExplainer(random_state=random_state)

        # Correctly construct the file path
        class_name = 'diseased'
        file_path = f'/kaggle/input/chest-xray-pneumonia/chest_xray/train/PNEUMONIA/person1000_virus_1681.jpeg{image_name}.JPG'

        img = cv2.imread(file_path, cv2.IMREAD_UNCHANGED)
        if img is None:
            raise FileNotFoundError(f"Unable to read the file at path: {file_path}")

        img = cv2.imread(file_path, cv2.IMREAD_UNCHANGED)
        img = cv2.resize(img, (IMAGE_SIZE[0], IMAGE_SIZE[1]))
        img = img / 255.0

        # Use model to predict
        prob = model.predict(tf.expand_dims(img, axis=0))
        prd = np.argmax(prob, axis=-1)

        # Visualization and Lime explanations
        plt.subplot(n_img, 4, id_img)
        plt.title('Original (Prediction = ' + str(prd) + ')')
        plt.ylabel(image_name + '.jpg')
        plt.imshow(img)
        id_img += 1

        # Segmentation and explanation
        plt.subplot(n_img, 4, id_img)
        plt.title('Segmentation')
        plt.imshow(mark_boundaries(img, segment_fn(img)))
        id_img += 1

        explanation = explainer.explain_instance(img, model.predict, num_samples=num_samples, segmentation_fn=segment_fn)
        temp, mask = explanation.get_image_and_mask(0, positive_only=False, num_features=5, hide_rest=False, min_weight=0.0)
        plt.subplot(n_img, 4, id_img)
        plt.title('Positive and Negative Regions')
        plt.imshow(mark_boundaries(temp, mask))
        id_img += 1

        temp, mask = explanation.get_image_and_mask(prd[0], positive_only=True if prd[0] == 1 else False, negative_only=True if prd[0] == 0 else False, num_features=1, hide_rest=False, min_weight=0.0)
        plt.subplot(n_img, 4, id_img)
        plt.title('Top ' + ('Positive' if round(prd[0]) == 1 else 'Negative') + ' Region')
        plt.imshow(mark_boundaries(temp, mask))
        id_img += 1

img_list = ['loh(16)','loh(18)','loh(10)','loh(23)',]
plt.rcParams['figure.figsize'] = [18, 5 * len(img_list)]
get_explanations(img_list, num_samples=DEFAULT_NUM_SAMPLES, random_state=0)

"""GramCam Not suitable for Transformer

"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions
from tensorflow.keras.preprocessing import image
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D
import cv2
from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.applications.densenet import DenseNet121
from tensorflow.keras.applications.resnet50 import ResNet50


# # Load a pre-trained VGG16 model
# base_model = VGG16(weights='imagenet')
# model = Model(inputs=base_model.input, outputs=base_model.output)



# Load a pre-trained DenseNet121 model
base_model = DenseNet121(weights='imagenet')
model = Model(inputs=base_model.input, outputs=base_model.output)

# # Load a pre-trained ResNet50 model
# base_model = ResNet50(weights='imagenet')
# model = Model(inputs=base_model.input, outputs=base_model.output)



#Function to preprocess the image
def process_img(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    img_array = image.img_to_array(img)
    img_array_expanded_dims = np.expand_dims(img_array, axis=0)
    return preprocess_input(img_array_expanded_dims)




# Grad-CAM algorithm
def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    # First, we create a model that maps the input image to the activations
    # of the last conv layer as well as the output predictions
    grad_model = tf.keras.models.Model(
        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]
    )

    # Then, we compute the gradient of the top predicted class for our input image
    # with respect to the activations of the last conv layer
    with tf.GradientTape() as tape:
        last_conv_layer_output, preds = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(preds[0])
        class_channel = preds[:, pred_index]

    # This is the gradient of the output neuron (top predicted or chosen)
    # with respect to the output feature map of the last conv layer
    grads = tape.gradient(class_channel, last_conv_layer_output)

    # This is a vector where each entry is the mean intensity of the gradient
    # over a specific feature map channel
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    # We multiply each channel in the feature map array
    # by "how important this channel is" with regard to the top predicted class
    # then sum all the channels to obtain the heatmap class activation
    last_conv_layer_output = last_conv_layer_output[0]
    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)

    # For visualization purpose, we will also normalize the heatmap between 0 & 1
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()

# Prepare image
img_path = '/kaggle/input/wheat-nitrogen-deficiency-and-leaf-rust-image/th422bg4yd-1/WheatLeafRust/test/diseased/34.jpg'  # Adjust this to the path of your image
# /kaggle/input/wheat-leaf-dataset/wheat_leaf/septoria/los(18).JPG
# /kaggle/input/wheat-nitrogen-deficiency-and-leaf-rust-image/th422bg4yd-1/WheatLeafRust/test/diseased/34.jpg
img_array = process_img(img_path)

# # Generate class activation heatmap
# last_conv_layer_name = 'block5_conv3'  # Last convolutional layer for VGG16
# heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)


# # Generate class activation heatmap using ResNet50's last conv layer
# last_conv_layer_name = 'conv5_block3_out'  # Appropriate last convolutional layer for ResNet50
# heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)

# Generate class activation heatmap using DenseNet121's last conv layer
last_conv_layer_name = 'relu'  # This is technically not a conv layer but the last activation layer suitable for Grad-CAM in DenseNet121
heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)

# Display heatmap
plt.matshow(heatmap)
plt.show()

def save_and_display_gradcam(img_path, heatmap, alpha=0.4):
    # Load the original image
    img = cv2.imread(img_path)
    img = cv2.resize(img, (224, 224))

    # Rescale heatmap to a range 0-255
    heatmap_rescaled = np.uint8(255 * heatmap)

    # Use jet colormap to colorize heatmap
    jet = cv2.applyColorMap(heatmap_rescaled, cv2.COLORMAP_JET)

    # Resize the heatmap to match the size of the original image
    jet = cv2.resize(jet, (img.shape[1], img.shape[0]))

    # Superimpose the heatmap on original image
    superimposed_img = jet * alpha + img
    superimposed_img = np.clip(superimposed_img, 0, 255).astype(np.uint8)

    # Pixelate the heatmap for middle image
    pixelated_heatmap = cv2.resize(jet, (32, 32), interpolation=cv2.INTER_NEAREST)
    pixelated_heatmap = cv2.resize(pixelated_heatmap, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)

    # Display Original, Pixelated Heatmap, and Original+Heatmap side by side
    fig, ax = plt.subplots(1, 3, figsize=(15, 5))

    # Original Image
    ax[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    ax[0].axis('off')
    ax[0].set_title('Original Image')

    # Pixelated Grad-CAM
    ax[1].imshow(cv2.cvtColor(pixelated_heatmap, cv2.COLOR_BGR2RGB))
    ax[1].axis('off')
    ax[1].set_title('Pixelated Grad-CAM')

    # Original + Heatmap
    ax[2].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    ax[2].imshow(cv2.cvtColor(jet, cv2.COLOR_BGR2RGB), alpha=alpha)  # Overlay the heatmap
    ax[2].axis('off')
    ax[2].set_title('Heatmap Over Original')

    plt.tight_layout()
    plt.show()

# Execute the function with the prepared image and heatmap
save_and_display_gradcam(img_path, heatmap)

from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input
from tensorflow.keras.preprocessing import image
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import cv2

# Load a pre-trained VGG19 model
base_model = VGG19(weights='imagenet')
model = Model(inputs=base_model.input, outputs=base_model.output)

# Function to preprocess the image
def process_img(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    img_array = image.img_to_array(img)
    img_array_expanded_dims = np.expand_dims(img_array, axis=0)
    return preprocess_input(img_array_expanded_dims)

# Grad-CAM algorithm adapted for VGG19
def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    grad_model = tf.keras.models.Model(
        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]
    )
    with tf.GradientTape() as tape:
        last_conv_layer_output, preds = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(preds[0])
        class_channel = preds[:, pred_index]
    grads = tape.gradient(class_channel, last_conv_layer_output)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    last_conv_layer_output = last_conv_layer_output[0]
    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()

# Prepare image
img_path = '/kaggle/input/wheat-nitrogen-deficiency-and-leaf-rust-image/th422bg4yd-1/WheatLeafRust/test/diseased/14.jpg'  # Adjust to your image path
img_array = process_img(img_path)

# Generate class activation heatmap using VGG19's last conv layer
last_conv_layer_name = 'block5_conv4'  # Appropriate last convolutional layer for VGG19
heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)

def save_and_display_gradcam(img_path, heatmap, alpha=0.4):
    img = cv2.imread(img_path)
    img = cv2.resize(img, (224, 224))
    heatmap_rescaled = np.uint8(255 * heatmap)
    jet = cv2.applyColorMap(heatmap_rescaled, cv2.COLORMAP_JET)
    jet = cv2.resize(jet, (img.shape[1], img.shape[0]))
    superimposed_img = jet * alpha + img
    superimposed_img = np.clip(superimposed_img, 0, 255).astype(np.uint8)

    pixelated_heatmap = cv2.resize(jet, (32, 32), interpolation=cv2.INTER_NEAREST)
    pixelated_heatmap = cv2.resize(pixelated_heatmap, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)

    fig, ax = plt.subplots(1, 3, figsize=(15, 5))
    ax[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    ax[0].axis('off')
    ax[0].set_title('Original Image')

    ax[1].imshow(cv2.cvtColor(pixelated_heatmap, cv2.COLOR_BGR2RGB))
    ax[1].axis('off')
    ax[1].set_title('Pixelated Grad-CAM')

    ax[2].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    ax[2].imshow(cv2.cvtColor(jet, cv2.COLOR_BGR2RGB), alpha=alpha)
    ax[2].axis('off')
    ax[2].set_title('Heatmap Over Original')

    plt.tight_layout()
    plt.show()

# Execute the function with the prepared image and heatmap
save_and_display_gradcam(img_path, heatmap)

from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input
from tensorflow.keras.preprocessing import image
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import cv2

# Load a pre-trained VGG19 model
base_model = VGG19(weights='imagenet')
model = Model(inputs=base_model.input, outputs=base_model.output)

# Function to preprocess the image
def process_img(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    img_array = image.img_to_array(img)
    img_array_expanded_dims = np.expand_dims(img_array, axis=0)
    return preprocess_input(img_array_expanded_dims)

# Grad-CAM algorithm adapted for VGG19
def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    grad_model = tf.keras.models.Model(
        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]
    )
    with tf.GradientTape() as tape:
        last_conv_layer_output, preds = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(preds[0])
        class_channel = preds[:, pred_index]
    grads = tape.gradient(class_channel, last_conv_layer_output)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    last_conv_layer_output = last_conv_layer_output[0]
    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()

# Prepare image
img_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/malignant/malignant (1).png'  # Adjust to your image path
img_array = process_img(img_path)

# Generate class activation heatmap using VGG19's last conv layer
last_conv_layer_name = 'block5_conv4'  # Appropriate last convolutional layer for VGG19
heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)

def save_and_display_gradcam(img_path, heatmap, alpha=0.4):
    img = cv2.imread(img_path)
    img = cv2.resize(img, (224, 224))
    heatmap_rescaled = np.uint8(255 * heatmap)
    jet = cv2.applyColorMap(heatmap_rescaled, cv2.COLORMAP_JET)
    jet = cv2.resize(jet, (img.shape[1], img.shape[0]))
    superimposed_img = jet * alpha + img
    superimposed_img = np.clip(superimposed_img, 0, 255).astype(np.uint8)

    pixelated_heatmap = cv2.resize(jet, (32, 32), interpolation=cv2.INTER_NEAREST)
    pixelated_heatmap = cv2.resize(pixelated_heatmap, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)

    fig, ax = plt.subplots(1, 3, figsize=(15, 5))
    ax[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    ax[0].axis('off')
    ax[0].set_title('Original Image')

    ax[1].imshow(cv2.cvtColor(pixelated_heatmap, cv2.COLOR_BGR2RGB))
    ax[1].axis('off')
    ax[1].set_title('Pixelated Grad-CAM')

    ax[2].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    ax[2].imshow(cv2.cvtColor(jet, cv2.COLOR_BGR2RGB), alpha=alpha)
    ax[2].axis('off')
    ax[2].set_title('Heatmap Over Original')

    plt.tight_layout()
    plt.show()

# Execute the function with the prepared image and heatmap
save_and_display_gradcam(img_path, heatmap)



import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import cv2
from skimage.segmentation import slic
from skimage.segmentation import mark_boundaries
from skimage.segmentation import felzenszwalb, slic, quickshift, watershed


def segment_fn(image):
    return slic(image, n_segments=50, compactness=10, sigma=1)

def get_explanations(image_names, num_samples=DEFAULT_NUM_SAMPLES, random_state=0):
    n_img = len(image_names)
    id_img = 1

    for image_name in image_names:
        explainer = lime_image.LimeImageExplainer(random_state=random_state)

        # Correctly construct the file path
        class_name = image_name.rsplit('_', 1)[0]  # Extract class name from image name
        file_path = f'/kaggle/input/bd-rice-leaf-dataset/Noised Field Background/{class_name}/{image_name}.jpg'

        img = cv2.imread(file_path, cv2.IMREAD_UNCHANGED)
        if img is None:
            raise FileNotFoundError(f"Unable to read the file at path: {file_path}")

        img = cv2.imread(file_path, cv2.IMREAD_UNCHANGED)
        img = cv2.resize(img, (IMAGE_SIZE[0], IMAGE_SIZE[1]))
        img = img / 255.0

        # Use model to predict
        prob = model.predict(tf.expand_dims(img, axis=0))
        prd = np.argmax(prob, axis=-1)

#         # Visualization and Lime explanations
#         plt.subplot(n_img, 4, id_img)
#         plt.title('Original (Prediction = ' + str(prd) + ')')
#         plt.ylabel(image_name + '.jpg')
#         plt.imshow(img)
#         id_img += 1

        # Segmentation and explanation
        plt.subplot(n_img, 4, id_img)
        plt.title('Segmentation')
        plt.imshow(mark_boundaries(img, segment_fn(img)))
        id_img += 1

#         explanation = explainer.explain_instance(img, model.predict, num_samples=num_samples, segmentation_fn=segment_fn)
# #         temp, mask = explanation.get_image_and_mask(0, positive_only=False, num_features=5, hide_rest=False, min_weight=0.0)
# #         plt.subplot(n_img, 4, id_img)
# #         plt.title('Positive and Negative Regions')
# #         plt.imshow(mark_boundaries(temp, mask))
# #         id_img += 1

#         temp, mask = explanation.get_image_and_mask(prd[0], positive_only=True if prd[0] == 1 else False, negative_only=True if prd[0] == 0 else False, num_features=1, hide_rest=False, min_weight=-0.5)
#         plt.subplot(n_img, 4, id_img)
#         plt.title('Top ' + ('Positive' if round(prd[0]) == 1 else 'Negative') + ' Region')
#         plt.imshow(mark_boundaries(temp, mask))
#         id_img += 1

img_list = ['Rice Blast_0009']
plt.rcParams['figure.figsize'] = [18, 5 * len(img_list)]
get_explanations(img_list, num_samples=DEFAULT_NUM_SAMPLES, random_state=0)

import cv2
import matplotlib.pyplot as plt
import tensorflow as tf

# Define the path to the specific image
image_path = '/kaggle/input/chest-xray-pneumonia/chest_xray/train/PNEUMONIA/person1001_bacteria_2932.jpeg'

# Load the image
img = cv2.imread(image_path, cv2.IMREAD_COLOR)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB

# Assuming IMAGE_SIZE is the size required by your model
IMAGE_SIZE = [112, 112]  # Replace with your model's input size

# Resize and normalize the image
image_resized = cv2.resize(img, (IMAGE_SIZE[0], IMAGE_SIZE[1]))
image_normalized = image_resized / 255.0

# Convert to tensor
image_tensor = tf.convert_to_tensor(image_normalized, dtype=tf.float32)
image_tensor = tf.expand_dims(image_tensor, 0)  # Add batch dimension

# Display the image
plt.imshow(image_normalized)
plt.axis('off')
plt.show()

import cv2
import matplotlib.pyplot as plt
import tensorflow as tf

# Define the path to the specific image
image_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/malignant/malignant (1).png'

# Load the image
img = cv2.imread(image_path, cv2.IMREAD_COLOR)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB

# Assuming IMAGE_SIZE is the size required by your model
IMAGE_SIZE = [112, 112]  # Replace with your model's input size

# Resize and normalize the image
image_resized = cv2.resize(img, (IMAGE_SIZE[0], IMAGE_SIZE[1]))
image_normalized = image_resized / 255.0

# Convert to tensor
image_tensor = tf.convert_to_tensor(image_normalized, dtype=tf.float32)
image_tensor = tf.expand_dims(image_tensor, 0)  # Add batch dimension

# Display the image
plt.imshow(image_normalized)
plt.axis('off')
plt.show()

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt

from skimage.color import rgb2gray
from skimage.filters import sobel
from skimage.segmentation import felzenszwalb, slic, quickshift, watershed
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float

segments_fz = felzenszwalb(img, scale=200, sigma=0.5, min_size=50)
segments_slic = slic(img, n_segments=50, compactness=10, sigma=1)
segments_quick = quickshift(img, kernel_size=2, max_dist=100, ratio=0.5)
gradient = sobel(rgb2gray(img))
segments_watershed = watershed(gradient, markers=25, compactness=0.001)

print(f"Felzenszwalb number of segments: {len(np.unique(segments_fz))}")
print(f"SLIC number of segments: {len(np.unique(segments_slic))}")
print(f"Quickshift number of segments: {len(np.unique(segments_quick))}")
print(f"Watershed number of segments: {len(np.unique(segments_watershed))}")
fig, ax = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)

ax[0, 0].imshow(mark_boundaries(img, segments_fz))
ax[0, 0].set_title("Felzenszwalbs's method")
ax[0, 1].imshow(mark_boundaries(img, segments_slic))
ax[0, 1].set_title('SLIC')
ax[1, 0].imshow(mark_boundaries(img, segments_quick))
ax[1, 0].set_title('Quickshift')
ax[1, 1].imshow(mark_boundaries(img, segments_watershed))
ax[1, 1].set_title('Compact watershed')

for a in ax.ravel():
    a.set_axis_off()

plt.tight_layout()
plt.show()

from skimage.color import rgb2gray
from skimage.filters import sobel
from skimage.segmentation import felzenszwalb, slic, quickshift, watershed
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float
from skimage.metrics import boundary_precision_recall
import numpy as np
import matplotlib.pyplot as plt

# Example image (replace with your own)
img = img_as_float(plt.imread('/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/malignant/malignant (10).png'))  # Replace 'your_image.png' with your actual image path

# Segment the image using different algorithms
segments_fz = felzenszwalb(img, scale=200, sigma=0.5, min_size=50)
segments_slic = slic(img, n_segments=50, compactness=10, sigma=1)
segments_quick = quickshift(img, kernel_size=2, max_dist=100, ratio=0.5)
gradient = sobel(rgb2gray(img))
segments_watershed = watershed(gradient, markers=25, compactness=0.001)

# Ground truth segmentation (replace with your ground truth if available)
ground_truth = np.zeros(img.shape[:2])  # Replace this with the actual ground truth mask

# Compute Boundary Precision (BP) and Boundary Recall (BR)
bp_fz, br_fz = boundary_precision_recall(segments_fz, ground_truth)
bp_slic, br_slic = boundary_precision_recall(segments_slic, ground_truth)
bp_quick, br_quick = boundary_precision_recall(segments_quick, ground_truth)
bp_watershed, br_watershed = boundary_precision_recall(segments_watershed, ground_truth)

# Print number of segments and BP/BR for each method
print(f"Felzenszwalb number of segments: {len(np.unique(segments_fz))}, BP: {bp_fz:.4f}, BR: {br_fz:.4f}")
print(f"SLIC number of segments: {len(np.unique(segments_slic))}, BP: {bp_slic:.4f}, BR: {br_slic:.4f}")
print(f"Quickshift number of segments: {len(np.unique(segments_quick))}, BP: {bp_quick:.4f}, BR: {br_quick:.4f}")
print(f"Watershed number of segments: {len(np.unique(segments_watershed))}, BP: {bp_watershed:.4f}, BR: {br_watershed:.4f}")

# Plot the segmentation results
fig, ax = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)

ax[0, 0].imshow(mark_boundaries(img, segments_fz))
ax[0, 0].set_title("Felzenszwalb's method")
ax[0, 1].imshow(mark_boundaries(img, segments_slic))
ax[0, 1].set_title('SLIC')
ax[1, 0].imshow(mark_boundaries(img, segments_quick))
ax[1, 0].set_title('Quickshift')
ax[1, 1].imshow(mark_boundaries(img, segments_watershed))
ax[1, 1].set_title('Compact watershed')

for a in ax.ravel():
    a.set_axis_off()

plt.tight_layout()
plt.show()





from skimage.color import rgb2gray
from skimage.filters import sobel
from skimage.segmentation import felzenszwalb, slic, quickshift, watershed
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float

segments_fz = felzenszwalb(img, scale=200, sigma=0.5, min_size=50)
segments_slic = slic(img, n_segments=50, compactness=10, sigma=1)
segments_quick = quickshift(img, kernel_size=2, max_dist=100, ratio=0.5)
gradient = sobel(rgb2gray(img))
segments_watershed = watershed(gradient, markers=25, compactness=0.001)

print(f"Felzenszwalb number of segments: {len(np.unique(segments_fz))}")
print(f"SLIC number of segments: {len(np.unique(segments_slic))}")
print(f"Quickshift number of segments: {len(np.unique(segments_quick))}")
print(f"Watershed number of segments: {len(np.unique(segments_watershed))}")
fig, ax = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)

ax[0, 0].imshow(mark_boundaries(img, segments_fz))
ax[0, 0].set_title("Felzenszwalbs's method")
ax[0, 1].imshow(mark_boundaries(img, segments_slic))
ax[0, 1].set_title('SLIC')
ax[1, 0].imshow(mark_boundaries(img, segments_quick))
ax[1, 0].set_title('Quickshift')
ax[1, 1].imshow(mark_boundaries(img, segments_watershed))
ax[1, 1].set_title('Compact watershed')

for a in ax.ravel():
    a.set_axis_off()

plt.tight_layout()
plt.show()

def get_explanations(image_names, num_samples=DEFAULT_NUM_SAMPLES, random_state=0):
    n_img = len(image_names) * 4  # 4 rows per image for each segmentation technique
    id_img = 1

    # Define segmentation functions in a list with their names
    segmentation_fns = [
        ('SLIC', lambda image: slic(image, n_segments=50, compactness=10, sigma=1)),
        ('Felzenszwalb', lambda image: felzenszwalb(image, scale=100, sigma=0.5, min_size=50)),
        ('Quickshift', lambda image: quickshift(image, kernel_size=3, max_dist=6, ratio=0.5)),
        ('Watershed', lambda image: watershed(sobel(rgb2gray(image)), markers=250, compactness=0.001))
    ]

    for image_name in image_names:
        explainer = lime_image.LimeImageExplainer(random_state=random_state)
        class_name = image_name.rsplit('_', 1)[0]
        file_path = f'/kaggle/input/chest-xray-pneumonia/{class_name}/{image_name}.jpg'

        img = cv2.imread(file_path, cv2.IMREAD_UNCHANGED)
        img = cv2.resize(img, (IMAGE_SIZE[0], IMAGE_SIZE[1]))
        img = img / 255.0

        prob = model.predict(tf.expand_dims(img, axis=0))
        prd = np.argmax(prob, axis=-1)

        for seg_name, segment_fn in segmentation_fns:
            # Apply segmentation
            segments = segment_fn(img)
            num_segments = len(np.unique(segments))

            # Visualization and Lime explanations
            plt.subplot(n_img, 4, id_img)
            plt.title(f'Original - {seg_name} (Pred = {prd})')
            plt.imshow(img)
            id_img += 1

            plt.subplot(n_img, 4, id_img)
            plt.title(f'{seg_name} Segmentation ({num_segments} segments)')
            plt.imshow(mark_boundaries(img, segments))
            id_img += 1

            explanation = explainer.explain_instance(img, model.predict, num_samples=num_samples, segmentation_fn=segment_fn)
            temp, mask = explanation.get_image_and_mask(prd[0], positive_only=False, num_features=5, hide_rest=False, min_weight=0.0)
            plt.subplot(n_img, 4, id_img)
            plt.title(f'{seg_name} Pos/Neg Regions')
            plt.imshow(mark_boundaries(temp, mask))
            id_img += 1

            temp, mask = explanation.get_image_and_mask(prd[0], positive_only=True if prd[0] == 1 else False, negative_only=True if prd[0] == 0 else False, num_features=1, hide_rest=False, min_weight=0.0)
            plt.subplot(n_img, 4, id_img)
            plt.title(f'{seg_name} Top Negative Region')
            plt.imshow(mark_boundaries(temp, mask))
            id_img += 1

# Usage
img_list = ['person1002_bacteria_2933']
plt.rcParams['figure.figsize'] = [18, 20 * len(img_list)]
get_explanations(img_list, num_samples=DEFAULT_NUM_SAMPLES, random_state=0)

import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import cv2
from skimage.segmentation import slic
from skimage.segmentation import mark_boundaries
from skimage.segmentation import felzenszwalb, slic, quickshift, watershed

def segment_fn(image):
    return slic(image, n_segments=50, compactness=10, sigma=1)

def get_explanations(image_names, num_samples=DEFAULT_NUM_SAMPLES, random_state=0):
    n_img = len(image_names) * 4  # 4 rows per image for each segmentation technique
    id_img = 1

    # Define segmentation functions in a list with their names
    segmentation_fns = [
        ('SLIC', lambda image: slic(image, n_segments=50, compactness=10, sigma=1)),
        ('Felzenszwalb', lambda image: felzenszwalb(image, scale=100, sigma=0.5, min_size=50)),
        ('Quickshift', lambda image: quickshift(image, kernel_size=3, max_dist=6, ratio=0.5)),
        ('Watershed', lambda image: watershed(sobel(rgb2gray(image)), markers=250, compactness=0.001))
    ]

    for image_name in image_names:
        explainer = lime_image.LimeImageExplainer(random_state=random_state)
        class_name = image_name.rsplit('_', 1)[0]
        file_path = f'/kaggle/input/chest-xray-pneumonia/{class_name}/{image_name}.jpeg'

        img = cv2.imread(file_path, cv2.IMREAD_UNCHANGED)
        if img is None:
            raise FileNotFoundError(f"Unable to read the file at path: {file_path}")

        # Resize the image only if it's loaded successfully
        img = cv2.resize(img, (IMAGE_SIZE[0], IMAGE_SIZE[1]))
        img = img / 255.0

        prob = model.predict(tf.expand_dims(img, axis=0))
        prd = np.argmax(prob, axis=-1)

        for seg_name, segment_fn in segmentation_fns:
            # Apply segmentation
            segments = segment_fn(img)
            num_segments = len(np.unique(segments))

            # Visualization and Lime explanations
            plt.subplot(n_img, 4, id_img)
            plt.title(f'Original - {seg_name} (Pred = {prd})')
            plt.imshow(img)
            id_img += 1

            plt.subplot(n_img, 4, id_img)
            plt.title(f'{seg_name} Segmentation ({num_segments} segments)')
            plt.imshow(mark_boundaries(img, segments))
            id_img += 1

            explanation = explainer.explain_instance(img, model.predict, num_samples=num_samples, segmentation_fn=segment_fn)
            temp, mask = explanation.get_image_and_mask(prd[0], positive_only=False, num_features=5, hide_rest=False, min_weight=0.0)
            plt.subplot(n_img, 4, id_img)
            plt.title(f'{seg_name} Pos/Neg Regions')
            plt.imshow(mark_boundaries(temp, mask))
            id_img += 1

            temp, mask = explanation.get_image_and_mask(prd[0], positive_only=True if prd[0] == 1 else False, negative_only=True if prd[0] == 0 else False, num_features=1, hide_rest=False, min_weight=0.0)
            plt.subplot(n_img, 4, id_img)
            plt.title(f'{seg_name} Top Negative Region')
            plt.imshow(mark_boundaries(temp, mask))
            id_img += 1

# Usage
img_list = ['person1003_bacteria_2934']
plt.rcParams['figure.figsize'] = [18, 20 * len(img_list)]
get_explanations(img_list, num_samples=DEFAULT_NUM_SAMPLES, random_state=0)



def get_explanations(image_names, num_samples=DEFAULT_NUM_SAMPLES, random_state=0):
    for image_name in image_names:
        explainer = lime_image.LimeImageExplainer(random_state=random_state)
        class_name = image_name.rsplit('_', 1)[0]
        file_path = f'/kaggle/input/bd-rice-leaf-dataset/Noised Field Background/{class_name}/{image_name}.jpg'

        img = cv2.imread(file_path, cv2.IMREAD_COLOR)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB
        img = cv2.resize(img, (IMAGE_SIZE[0], IMAGE_SIZE[1]))
        img = img / 255.0

        prob = model.predict(tf.expand_dims(img, axis=0))
        prd = np.argmax(prob, axis=-1)

        # SLIC segmentation
        segments = slic(img, n_segments=50, compactness=10, sigma=1)
        explanation = explainer.explain_instance(img, model.predict, num_samples=num_samples, segmentation_fn=lambda x: segments)
        temp, mask = explanation.get_image_and_mask(prd[0], positive_only=True, num_features=5, hide_rest=False, min_weight=0.01)

        # Create a mask overlay for diseased segments
        mask_overlay = np.zeros_like(img)
        for segment_id in np.unique(segments):
            if mask[segments == segment_id].any():
                mask_overlay[segments == segment_id] = [1, 0, 0]  # Red color

        # Apply the mask overlay
        labeled_img = img.copy()
        labeled_img[mask_overlay.sum(axis=-1) > 0] = labeled_img[mask_overlay.sum(axis=-1) > 0] * 0.5 + mask_overlay[mask_overlay.sum(axis=-1) > 0] * 0.5

        # Visualization
        plt.figure(figsize=(18, 6))
        plt.subplot(1, 3, 1)
        plt.title('Original')
        plt.imshow(img)
        plt.axis('off')

        plt.subplot(1, 3, 2)
        plt.title('Segmentation')
        plt.imshow(mark_boundaries(img, segments))
        plt.axis('off')

        plt.subplot(1, 3, 3)
        plt.title('Diseased Segments')
        plt.imshow(labeled_img)
        plt.axis('off')

        plt.show()

# Usage
img_list = ['Browon Spot_0001']
get_explanations(img_list, num_samples=DEFAULT_NUM_SAMPLES, random_state=0)

import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import cv2
from skimage.segmentation import slic
from skimage.segmentation import mark_boundaries
from skimage import color

def get_explanations(image_names, num_samples=DEFAULT_NUM_SAMPLES, random_state=0):
    for image_name in image_names:
        explainer = lime_image.LimeImageExplainer(random_state=random_state)
        class_name = image_name.rsplit('_', 1)[0]
        file_path = f'/kaggle/input/bd-rice-leaf-dataset/Noised Field Background/{class_name}/{image_name}.jpg'

        img = cv2.imread(file_path, cv2.IMREAD_COLOR)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB
        img = cv2.resize(img, (IMAGE_SIZE[0], IMAGE_SIZE[1]))
        img = img / 255.0

        prob = model.predict(tf.expand_dims(img, axis=0))
        prd = np.argmax(prob, axis=-1)

        # SLIC segmentation
        segments = slic(img, n_segments=50, compactness=10, sigma=1)
        explanation = explainer.explain_instance(img, model.predict, num_samples=num_samples, segmentation_fn=lambda x: segments)
        temp, mask = explanation.get_image_and_mask(prd[0], positive_only=True, num_features=5, hide_rest=False, min_weight=0.01)

        # Create a mask overlay for diseased segments
        mask_overlay = np.zeros_like(img)
        for segment_id in np.unique(segments):
            if mask[segments == segment_id].any():
                mask_overlay[segments == segment_id] = [1, 0, 0]  # Red color

        # Apply the mask overlay
        labeled_img = img.copy()
        labeled_img[mask_overlay.sum(axis=-1) > 0] = labeled_img[mask_overlay.sum(axis=-1) > 0] * 0.5 + mask_overlay[mask_overlay.sum(axis=-1) > 0] * 0.5

        # Calculate the total leaf area
        gray_img = color.rgb2gray(img)
        leaf_mask = gray_img > 0.3 # Threshold to identify the leaf
        total_leaf_area = np.sum(leaf_mask)

        # Calculate diseased area (number of red pixels in the mask_overlay)
        diseased_area = np.sum(mask_overlay[:, :, 0] == 1)

        # Calculate the percentage of leaf area diseased
        percentage_diseased = (diseased_area / total_leaf_area) * 100

        # Visualization
        plt.figure(figsize=(18, 6))
        plt.subplot(1, 3, 1)
        plt.title('Original')
        plt.imshow(img)
        plt.axis('off')

        plt.subplot(1, 3, 2)
        plt.title('Segmentation')
        plt.imshow(mark_boundaries(img, segments))
        plt.axis('off')

        plt.subplot(1, 3, 3)
        plt.title(f'Diseased Segments - {percentage_diseased:.2f}% of leaf area')
        plt.imshow(labeled_img)
        plt.axis('off')

        plt.show()

# Usage
img_list = ['Browon Spot_0001']
# Start timing
start_time = time.process_time()
get_explanations(img_list, num_samples=DEFAULT_NUM_SAMPLES, random_state=0)
end_time = time.process_time()
print(f"CPU time for get_explanations: {end_time - start_time} seconds")

import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import cv2
from skimage.segmentation import watershed
from skimage.filters import sobel
from skimage import color
from skimage.measure import label, regionprops

def get_explanations(image_names, num_samples=DEFAULT_NUM_SAMPLES, random_state=0):
    for image_name in image_names:
        explainer = lime_image.LimeImageExplainer(random_state=random_state)
        class_name = image_name.rsplit('_', 1)[0]
        file_path = f'/kaggle/input/bd-rice-leaf-dataset/Noised Field Background/{class_name}/{image_name}.jpg'

        img = cv2.imread(file_path, cv2.IMREAD_COLOR)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB
        img = cv2.resize(img, (IMAGE_SIZE[0], IMAGE_SIZE[1]))
        img = img / 255.0

        prob = model.predict(tf.expand_dims(img, axis=0))
        prd = np.argmax(prob, axis=-1)

        # Watershed segmentation
        gradient = sobel(rgb2gray(img))
        segments = watershed(gradient, markers=250, compactness=0.001)

        explanation = explainer.explain_instance(img, model.predict, num_samples=num_samples, segmentation_fn=lambda x: segments)
        temp, mask = explanation.get_image_and_mask(prd[0], positive_only=True, num_features=5, hide_rest=False, min_weight=0.01)

        # Overlay explanation mask on the original image
        highlighted_img = img.copy()
        highlighted_img[mask == 1] = [1, 0, 0]  # Highlight the influenced regions in red

        # Visualization
        plt.figure(figsize=(18, 6))
        plt.subplot(1, 3, 1)
        plt.title('Original')
        plt.imshow(img)
        plt.axis('off')

        plt.subplot(1, 3, 2)
        plt.title('Segmentation')
        plt.imshow(mark_boundaries(img, segments))
        plt.axis('off')

        plt.subplot(1, 3, 3)
        plt.title('Influenced Regions')
        plt.imshow(highlighted_img)
        plt.axis('off')

        plt.show()

# Usage
img_list = ['Browon Spot_0001']
# Start timing
start_time = time.process_time()
get_explanations(img_list, num_samples=DEFAULT_NUM_SAMPLES, random_state=0)
end_time = time.process_time()
print(f"CPU time for get_explanations: {end_time - start_time} seconds")